---
layout: post
title: SARSA vs. Q-learning
---

SARSA stands for "state-action-reward-state-action." In Q-learning, the agent starts out in state S, performs action A, sees what the highest possible reward is for taking any action from its new state T, and updates its value for the state S-action A pair based on this new highest possible value. In SARSA, the agent starts in state S, takes action A and gets a reward, moves to state T, takes action B and gets a reward, and goes back to update the the value for S-A based on the actual value of the reward it received from taking action B. 

The result is that Q-learning assumes that the agent is following the best possible policy without attempting to resolve what that policy actually is, while SARSA takes into account the agent's actual policy (what it ends up doing when it moves to the next state as opposed to the best possible thing it could be assumed to do).

As mentioned earlier in the chapter, Q-learning and SARSA are very similar algorithms, and in fact Q-learning is sometimes called SARSA-max. When the agent's policy is simply the greedy one (choose the highest-valued action from the next state no matter what), Q-learning and SARSA will produce the same results. 
