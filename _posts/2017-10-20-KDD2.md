---
layout: post
title: KDD algebra 2005-2006 data 
---

The 2005-2006 algebra training set, for context, contains 112 unique knowledge component strings. Transactions containing a single knowledge component make up just over 50% of the dataset (about 400,000 observations). Because 80% of these transactions have positive target values (Correct First Attempt = 1) a fully na√Øve model that labeled every target value as positive would be guaranteed to misclassify 20% of these transactions, or 10% of the dataset.

There is additionally a negative correlation between the number of knowledge components in a transaction and the likelihood of a correct answer, from about 80% for transactions with zero, one, or two components down to about 40% for transactions with five or more components. 

The number of knowledge components therefore represents a conceptual axis (in practice known as a feature) that can reasonably be called something like "problem complexity." It's not necessary to label your axes in this way for the benefit of most machine learning models, which are capable of making a connection this straightforward on their own. (Engineering simple features like this *can* be helpful when you want to do something like strengthen a signal by binning noisy values.) It is, however, a good way to clarify for yourself what relationships you want to be searching for within your data when you're formulating additional research questions.

"Problem complexity," or at least this one aspect of it, is my first extraction, the second relating to the number of times a student has encountered the same category of problem, which might be called something like "exposure level." (I don't want to tie this to skill level because student improvement tends to peak at about 40-50 attempts at a knowledge component, after which it reverts to where it started. So they're only getting better up to a point.)
